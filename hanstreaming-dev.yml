# data-collector module configuration
data-collector:
  # for querying datasource configuration
  elastic:
    cluster-name: "es-jw-darpa"
    cluster-nodes: "172.16.150.172"
    tcp-port: "29300"
    # This instructs the sink to emit after evert element, otherwise they would be buffered
    buffer-size: "1"
    query-scroll-size: 1000
    # scroll context keep time
    keep-context-alive-sec: 120

  kafka:
    bootstrap.servers: "172.16.150.189:9092"
    zookeeper: "172.16.150.189:2181"

  sharded-jedis:
    hosts: "172.16.150.189:6379"
    pool_config:
      max_total: 64
      max_idle: 64
      min_idle: 16
      test_while_idle: true
      # 15 seconds
      max_wait_mills: 15000

  app:
    server.port: 9989
    scratch-interval-sec: 10
    slice-size: 1800000 # half of an hour

    time_field: "@timestamp"
    user_field: "user"
    group_fields: ["role", "departement"]

    config-refresh-interval-sec: 5
    max-data-source-count: 100

    data_range:
      from: 1522512000000
      # default 3000/00/00 00:00:00
      to: 32503651200000

hanstreaming-ml:
  flink:
    window:
      # 4h  feature generate window
      profile-size: 14400
      # 15m for watermark calculate
      out-of-orderness: 1800
      # 1d  algorithm window
      window-size: 86400

  elastic:
    cluster-name: "es-jw-darpa"
    cluster-nodes: "172.16.150.125:29300" # mulity nodes split by ','
    buffer-size: "1"
    indices-pattern: "event_*"
    type-pattern: "event" # mulity type split by ','

  rocksdb:
    db-dir: "/tmp/rocksdb_db/"

  app:
    user_field: "userId"
    rule:
      weight: 10

  redis:
    host: "172.16.150.125"
    port: 6379
  # max connection count, default 8
    maxTotal: 20
  # max idle connection count, default 8
    maxIdle: 5
  # max wait mills for getting a connection, raise exception when time out. default -1.
    maxWaitMills: 6000
  # max wait mills for getting a pool
    pool.maxWaitMills: 2000
  # retry times when cannot get connection from pool
    client.retryTimes: 5

streaming:
  elastic:
    cluster-name: "es-jw-darpa"
    cluster-nodes: "172.16.150.125:29300" # mulity nodes split by ','

  redis:
    host: "172.16.150.189"
    port: 6379
    # max connection count, default 8
    maxTotal: 128
    # max idle connection count, default 8
    maxIdle: 16
    # max wait mills for getting a connection, raise exception when time out. default -1.
    maxWaitMills: 5000
    # max wait mills for getting a pool
    pool.maxWaitMills: 2000
    # retry times when cannot get connection from pool
    client.retryTimes: 5

  flink:
    checkpoints-dir: "file:///tmp/flink/checkpoints"

    window:
      # 1h
      profile-size: 3600
      # 15m, for watermark calculate
      out-of-orderness: 1800
      # 1d
      window-size: 86400
      # 1h
      slide-size: 3600

  app:
    user_field: "userId"

    # detector default parameters
    detector:
      SlidingWindowAnomalyDetector:
        threshold: 0.8
        slicesPerWindow: 4

      GeoBaselineAnomalyDetector:
        # distance in kilometer
        M: 500

      FeatureBasedAnomalyDetector:
        threshold: 0.1
        topN: -1

      FeatureBasedAnomalyDetectorAcrossGroup:
        threshold: 0.11
        topN: -1

  # visualizer & rule template mapping
  visualizer:
    # new relationship
    a4c33fa1-226a-426c-88f3-76297a73fa25: NewRelVisualizer
    # n relationships occured in m minutes
    eb3646f4-d946-4303-8ace-3fc608b1c085: NewRelVisualizer
    # distance between two events
    af3ad877-18a4-42c2-a0ad-6751fc6b0a52: GeoPointVisualizer
    # deviate regualr area
    f02c7c50-24f4-45c1-b63d-30658c2e3050: AlgoGeoPointVisualizer
    # count rules
    de3f5601-03dc-4467-8578-5f49bf5d03b3: SimpleThresholdVisualizer

    # 1400a77b-afcd-4971-bb6c-9307d3422c06: SimpleThresholdVisualizerForRaw
    # staff baseline
    a70e7377-c30d-4e31-b08b-4c8d4d909c7b: SimpleThresholdVisualizer
    # department baseline
    9d7c0853-8539-4288-a683-8f75d7277727: SimpleThresholdVisualizer

  rule:
    weight: 10

  geo_hash.character_precision: 3

web:
  server:
    port: 8088
  spring:
    application:
      name: hanstreaming
    http:
      encoding:
        enabled: true
        charset: UTF-8
      multipart:
        max-file-size: 1000MB
        max-request-size: 1000MB

  # zuul:
  #  host:
  #    connect-timeout-millis: 120000
  #    socket-timeout-millis: 120000
  #  routes:
  #    ueba:
  #      path: /ueba/**
  #      url: http://172.16.150.31:8080
  #      strip-prefix: true

  # es:
  #  cluster-name: es-jw-darpa
  #  hosts: ["172.16.150.31"]
  #  port: 29300

  collector:
    host: 172.16.150.189
    port: 9989

  hal:
    host: 172.16.150.149
    port: 8085

  flink:
    cluster: 0.0.0.0:6123
    run: /usr/local/bin/flink
  #  run: /home/sonice/Applications/flink/bin/flink
    task-manager-host: 0.0.0.0
    state-server-proxy-port: 9069
    endpoint: 0.0.0.0:8081

  kafka:
    server: 172.16.150.189:9092
    zookeeper: 172.16.150.189:2181
    group: ueba-event-process-
    group-test: ueba-event-process-test-

  cmd:
    javac: /bin/javac

  # dynamic compilation output directory
  output: output

  es:
    cluster.name: "es-jw-darpa"
    hosts: ["172.16.150.172"]
    http.port: "29200"
    transport.tcp.port: "29300"

  redis:
    host: "172.16.150.189"
    port: "6379"
